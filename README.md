# ğŸ–¼ï¸ğŸ§  Image Recognition Chatbot using Flask + ONNX + Ollama

This project combines image captioning, object classification (dog, bird, flower, landmark), and a conversational AI chatbot powered by Ollama (local LLM like Mistral/Gemma/LLaMA).

ğŸ”¥ Features

ğŸ§  AI chatbot powered by Ollama (local LLM like Mistral/Gemma)

ğŸ–¼ï¸ Image captioning using microsoft/git-large-textcaps

ğŸ¶ Image classification using ONNX models for:

Dogs ğŸ•
Birds ğŸ¦
Flowers ğŸŒ¸
Landmarks ğŸ°

ğŸ§¾ Session-aware conversation with context from uploaded image
ğŸ”§ Built with Flask, Transformers, ONNX Runtime, Torch, and Ollama

ğŸ“ Folder Structure
.
â”œâ”€â”€ app.py / run.py               # Flask app
â”œâ”€â”€ models/                       # ONNX model files (flower.onnx, dog.onnx, etc.)
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ index.html                # Frontend HTML
â”œâ”€â”€ static/                       # Optional CSS/JS
â”œâ”€â”€ venv/                         # Virtual environment
â””â”€â”€ README.md

ğŸš€ Setup Instructions

1. ğŸ”ƒ Clone the Repo

git clone https://github.com/prathamj937/image-recognition-chatbot.git
cd image-recognition-chatbot

2. ğŸ Create Virtual Environment

python -m venv venv
source venv/bin/activate        # On Windows: venv\Scripts\activate

3. ğŸ“¦ Install Dependencies

pip install -r requirements.txt

If requirements.txt is not created, manually install:

pip install flask transformers torch onnxruntime pillow numpy ollama

4. ğŸ“¥ Download or Place ONNX Models

Put the following models in the models/ folder:

flower.onnx
dog.onnx
bird.onnx
landmark.onnx

(You can export these from your training script or request them from the developer.)

5. ğŸ§  Install & Run Ollama

Install Ollama and pull a lightweight model:

ollama pull mistral     # or gemma:2b for lower memory usage

Keep the Ollama server running in a separate terminal:

ollama serve

6. ğŸš¦ Run the Flask App

python run.py

Visit http://localhost:5000 in your browser.

ğŸ§  Chatbot Model Selection (Ollama)

In get_ollama_response() function (inside run.py), you can switch model:

response = ollama.chat(model="mistral", messages=messages)

Use:
mistral
gemma:2b
llama3 (only if enough RAM)

ğŸ’¬ Example Flow

Upload image of a dog ğŸ¶
App captions: "A cute dog sitting in the grass."
Dog breed classification result: "Golden Retriever"
Ask chatbot: "Tell me more about this breed."
Bot answers using local LLM via Ollama ğŸ§ 

âš™ï¸ Troubleshooting





Ollama model not found â†’ Run ollama pull mistral



"requires more system memory" â†’ Use lighter model like mistral or gemma:2b
